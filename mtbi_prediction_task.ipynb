{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4fba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f0dd2",
   "metadata": {},
   "source": [
    "Load the data and do some quick analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bb0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_data = pd.read_csv('data_archive.zip')\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab4e8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['type'].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826351d0",
   "metadata": {},
   "source": [
    "The dataset is heavily unbalanced. So, I decided to first attempt classification for the two most frequent classes only: INTJ and INTP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc14ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = all_data[all_data['type'].isin(['INTP', 'INTJ'])]\n",
    "num_records_of_each_type = 100\n",
    "intp_data = all_data[all_data['type'].isin(['INTP'])].head(num_records_of_each_type)\n",
    "intj_data = all_data[all_data['type'].isin(['INTJ'])].head(num_records_of_each_type)\n",
    "data = pd.concat([intp_data, intj_data])\n",
    "data\n",
    "data['type'].value_counts()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['posts'], data['type'], test_size=0.1)\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241832d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "import mlflow\n",
    "import sys, json\n",
    "from joblib import dump, load\n",
    "\n",
    "neptune_project = neptune.init_project(name=\"tapadipti/mtbi\", api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2MWI2YzlkMS0zNjBlLTQ1NjEtYmUxNS05MDI1ZGMyNDA1ODAifQ==\")\n",
    "\n",
    "neptune_run = neptune.init(\n",
    "    project=\"tapadipti/mtbi\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI2MWI2YzlkMS0zNjBlLTQ1NjEtYmUxNS05MDI1ZGMyNDA1ODAifQ==\",\n",
    "    source_files=[\"mtbi_prediction_task.ipynb\"]\n",
    ")\n",
    "\n",
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_experiment(\"/Users/tapadipti@gmail.com/mtbi_personality_types\")\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "mlflow.start_run()\n",
    "\n",
    "with open(\"params.json\") as f:\n",
    "    all_params = json.load(f)\n",
    "params = all_params[\"lr\"]\n",
    "\n",
    "lr = LogisticRegression(**params)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "with open(\"metrics.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"accuracy\": accuracy}))\n",
    "dump(lr, \"model.joblib\")\n",
    "\n",
    "neptune_run[\"parameters\"] = params\n",
    "neptune_run[\"accuracy\"] = accuracy\n",
    "\n",
    "neptune_run[\"accuracy_log\"].log(accuracy)\n",
    "\n",
    "neptune_run[\"parameters\"]\n",
    "neptune_run[\"accuracy\"]\n",
    "\n",
    "\n",
    "mlflow.log_param(\"penalty\", params[\"penalty\"])\n",
    "mlflow.log_metric(\"accuracy\", accuracy)\n",
    "# mlflow.log_metric(\"accuracy\", accuracy+0.01)\n",
    "# mlflow.log_metric(\"accuracy\", accuracy+0.02)\n",
    "from urllib.parse import urlparse\n",
    "tracking_uri_scheme = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "# if tracking_uri_scheme != \"file\":\n",
    "mlflow.sklearn.log_model(lr, \"model\")\n",
    "# else:\n",
    "#     mlflow.sklearn.log_model(lr, \"model\")\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm\n",
    "\n",
    "neptune_project[\"general/source_code\"].upload(\"mtbi_prediction_task.ipynb\")\n",
    "neptune_project[\"dataset/v0.1\"].track_files(\"./data_archive.zip\")\n",
    "\n",
    "neptune_run[\"notebook_code\"].track_files(\"mtbi_prediction_task.ipynb\")\n",
    "neptune_run[\"train_dataset\"].track_files(\"./data_archive.zip\")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "\n",
    "# ax = sns.heatmap(cm, cmap=\"Blues\", annot=True, fmt=\"d\")\n",
    "\n",
    "# ax.set_title('Confusion Matrix');\n",
    "# ax.set_xlabel('Predicted Type')\n",
    "# ax.set_ylabel('Actual Type');\n",
    "\n",
    "# ax.xaxis.set_ticklabels(['INTJ','INTP'])\n",
    "# ax.yaxis.set_ticklabels(['INTJ','INTP'])\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Types')\n",
    "plt.ylabel('Actual Types')\n",
    "\n",
    "ax.matshow(cm, cmap=\"Blues\", alpha=0.2)\n",
    "\n",
    "mtbi_types = ['INTJ', 'INTP']\n",
    "tick_positions = np.arange(len(mtbi_types))\n",
    "ax.xaxis.set_ticks(tick_positions)\n",
    "ax.xaxis.set_ticklabels(mtbi_types)\n",
    "ax.yaxis.set_ticks(tick_positions)\n",
    "ax.yaxis.set_ticklabels(mtbi_types)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='large')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "neptune_run[\"confusion_matrix\"].upload(fig)\n",
    "neptune_run.stop()\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03d082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# neptune_run[\"precision\"] = {\"INTJ\": 0.92, \"INTP\": 0.92}\n",
    "# neptune_run[\"recall\"] = {\"INTJ\": 0.91, \"INTP\": 0.93}\n",
    "# neptune_run[\"f1-score\"] = {\"INTJ\": 0.91, \"INTP\": 0.93}\n",
    "\n",
    "\n",
    "# neptune_project[\"general/brief\"] = \"URL_TO_PROJECT_BRIEF\"\n",
    "\n",
    "# project[\"dataset/latest\"] = project[\"dataset/v0.1\"].fetch()\n",
    "# project = neptune.init_project(\"tapadipti/mtbi\", mode=\"read-only\")\n",
    "# run[\"dataset\"] = project[\"dataset/v0.1\"].fetch()\n",
    "# run[\"dataset\"].download()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97144336",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
